{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11520303,"sourceType":"datasetVersion","datasetId":7225055}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clear output folder\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working/vqvae_checkpoints'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:39:29.798458Z","iopub.execute_input":"2025-04-23T01:39:29.799205Z","iopub.status.idle":"2025-04-23T01:39:29.831876Z","shell.execute_reply.started":"2025-04-23T01:39:29.799178Z","shell.execute_reply":"2025-04-23T01:39:29.831179Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Come Usarli:\n\nEsegui Script 1 (preprocess_audio.py):\n\nMettilo in una cella del notebook Kaggle.\n\nAssicurati che Config.mp3_dir punti ai tuoi MP3 e Config.preprocessed_dir sia dove vuoi salvare i file .pt (es. /kaggle/working/preprocessed_spectrograms/).\n\nEseguilo. Aspetta che finisca (potrebbe richiedere tempo). Controlla che la cartella di output contenga molti file .pt.\n\nEsegui Script 2 (train_vqvae.py):\n\nMettilo in una nuova cella del notebook (o riavvia il kernel se necessario).\n\nVerifica la Configurazione: Assicurati che i parametri audio/spettrogramma in Config siano identici a quelli usati nello Script 1. Imposta i parametri di training (batch_size, learning_rate, use_mixed_precision, num_epochs, etc.) come desideri per questa sessione di training. Importante: use_mixed_precision è impostato su False qui, come nella tua ultima esecuzione stabile. Se vuoi provare a riabilitarlo, cambialo in True.\n\nAssicurati che Config.preprocessed_dir punti alla cartella creata dallo Script 1.\n\nAssicurati che Config.checkpoint_dir sia dove vuoi salvare/caricare i checkpoint del modello.\n\nEseguilo. Cercherà automaticamente l'ultimo checkpoint nella checkpoint_dir e riprenderà da lì. Se non ne trova, inizierà dall'epoca 0.\n\nOra hai un flusso di lavoro pulito con preprocessing e training separati.","metadata":{}},{"cell_type":"code","source":"# #      VQ-VAE - SCRIPT 1: PREPROCESSING     #\n# # (Loads MP3s, Creates Spectrograms, Saves .pt) #\n# ##############################################\n\n# --- 1. Imports ---\nprint(\"Importing libraries for preprocessing...\")\nimport torch\n# No need for nn, optim here\nimport torchaudio\nfrom dataclasses import dataclass, field\nimport typing as T\nimport numpy as np\nimport io\nimport os\nimport glob\nimport time\nimport warnings\nimport traceback\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport pydub # For MP3 loading\nfrom scipy.io import wavfile\n\n# --- 2. Configuration ---\n# !! Ensure these parameters match what you'll use for training !!\nprint(\"Setting up configuration...\")\n\n@dataclass\nclass Config:\n    # --- Paths ---\n    # !! MODIFY THIS !! Input directory containing MP3 files\n    mp3_dir: Path = Path(\"/kaggle/input/demoscene-mp3s/mp3 2/\")\n    # !! MODIFY THIS !! Output directory for preprocessed spectrogram chunks\n    preprocessed_dir: Path = Path(\"/kaggle/working/preprocessed_spectrograms/\")\n    # Checkpoint directory - Not used here, but kept for consistency\n    checkpoint_dir: Path = Path(\"/kaggle/working/vqvae_checkpoints/\")\n\n    # --- Preprocessing & Spectrogram Parameters ---\n    target_sample_rate: int = 22050\n    chunk_duration_ms: int = 2000 # Duration of each training chunk\n    step_size_ms: int = 20\n    window_duration_ms: int = 50\n    padded_duration_ms: int = 200 # Affects n_fft\n    num_frequencies: int = 128 # Mel bins\n    min_frequency: int = 30\n    max_frequency: int = 8000\n    mel_scale_norm: T.Optional[str] = None # e.g., \"slaney\"\n    mel_scale_type: str = \"htk\"\n    power_for_spectrogram: float = 2.0 # Use power=2 for power spectrogram\n    # Griffin-Lim parameters (only needed if reconstructing audio directly here)\n    # num_griffin_lim_iters: int = 32\n\n    # --- VQ-VAE Model Parameters (Not needed for preprocessing) ---\n    # embedding_dim: int = 64\n    # num_embeddings: int = 512\n    # num_hiddens: int = 128\n    # num_residual_layers: int = 2\n    # num_residual_hiddens: int = 32\n    # commitment_cost: float = 0.25\n\n    # --- Training Parameters (Not needed for preprocessing) ---\n    # num_epochs: int = 25\n    # batch_size: int = 256 # Increased based on last run\n    # learning_rate: float = 2e-4\n    # use_mixed_precision: bool = False # Set based on last stable run\n    # gradient_clip_val: float = 1.0\n    # device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # num_workers: int = 0\n    # pin_memory: bool = False\n    # print_every_n_batches: int = 50\n    # save_every_n_epochs: int = 1\n\n    # --- Derived Parameters (Calculated automatically) ---\n    chunk_length_samples: int = field(init=False)\n    n_fft: int = field(init=False)\n    win_length: int = field(init=False)\n    hop_length: int = field(init=False)\n    num_time_steps_per_chunk: int = field(init=False) # Expected time dimension\n\n    def __post_init__(self):\n        # Calculate derived parameters after initialization\n        self.chunk_length_samples = int(self.chunk_duration_ms / 1000.0 * self.target_sample_rate)\n        self.n_fft = int(self.padded_duration_ms / 1000.0 * self.target_sample_rate)\n        self.win_length = int(self.window_duration_ms / 1000.0 * self.target_sample_rate)\n        self.hop_length = int(self.step_size_ms / 1000.0 * self.target_sample_rate)\n        self.num_time_steps_per_chunk = (self.chunk_length_samples // self.hop_length) + 1\n        print(f\"Derived Config: Chunk Samples={self.chunk_length_samples}, n_fft={self.n_fft}, hop={self.hop_length}, win={self.win_length}, T={self.num_time_steps_per_chunk}\")\n        print(f\"Preprocessing Target SR: {self.target_sample_rate}\")\n\n# Instantiate Configuration\ncfg = Config()\n\n# --- 3. Utility Functions & Classes ---\nprint(\"Defining utilities for preprocessing...\")\n\n# (audio_from_waveform is not strictly needed but harmless to keep)\ndef audio_from_waveform(samples: np.ndarray, sample_rate: int, normalize: bool = False) -> pydub.AudioSegment:\n    if samples.ndim == 1: samples = samples[np.newaxis, :]\n    if samples.shape[0] != 1: samples = samples[0:1, :]\n    if normalize:\n        max_val = np.max(np.abs(samples));\n        if max_val > 1e-6: samples = samples / max_val * 0.95\n    samples_int16 = (samples * np.iinfo(np.int16).max).astype(np.int16)\n    samples_int16_transposed = samples_int16.transpose()\n    wav_bytes = io.BytesIO(); wavfile.write(wav_bytes, sample_rate, samples_int16_transposed); wav_bytes.seek(0)\n    return pydub.AudioSegment.from_wav(wav_bytes)\n\nclass SpectrogramConverter:\n    \"\"\"Handles conversion between waveforms and Mel spectrograms using torchaudio.\"\"\"\n    def __init__(self, cfg: Config, device: str):\n        self.cfg = cfg; self.device = torch.device(device)\n        print(f\"SpectrogramConverter using device: {self.device}\")\n        self.spectrogram_func = torchaudio.transforms.Spectrogram(n_fft=cfg.n_fft, hop_length=cfg.hop_length, win_length=cfg.win_length, power=cfg.power_for_spectrogram, center=True, pad_mode=\"reflect\", onesided=True, window_fn=torch.hann_window).to(self.device)\n        self.mel_scaler = torchaudio.transforms.MelScale(n_mels=cfg.num_frequencies, sample_rate=cfg.target_sample_rate, f_min=cfg.min_frequency, f_max=cfg.max_frequency, n_stft=cfg.n_fft // 2 + 1, norm=cfg.mel_scale_norm, mel_scale=cfg.mel_scale_type).to(self.device)\n        self.db_scaler = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80.0).to(self.device)\n        self.min_db_level = -80.0\n\n    def _normalize_spectrogram(self, mel_spec_db: torch.Tensor) -> torch.Tensor:\n        mel_spec_db_clamped = torch.clamp(mel_spec_db, min=self.min_db_level); normalized = (mel_spec_db_clamped - self.min_db_level) / (-self.min_db_level); return torch.clamp(normalized, 0.0, 1.0)\n\n    def spectrogram_from_waveform_tensor(self, waveform_tensor: torch.Tensor) -> torch.Tensor:\n        if waveform_tensor.device != self.device: waveform_tensor = waveform_tensor.to(self.device)\n        with torch.no_grad(): spectrogram_pow = self.spectrogram_func(waveform_tensor); mel_spec_pow = self.mel_scaler(spectrogram_pow); mel_spec_db = self.db_scaler(mel_spec_pow); mel_spec_db_normalized = self._normalize_spectrogram(mel_spec_db)\n        return mel_spec_db_normalized\n\n# --- 4. Preprocessing Function ---\nprint(\"Defining Preprocessing Function...\")\n\ndef preprocess_mp3s(cfg: Config):\n    \"\"\"Loads MP3s, converts, chunks, creates spectrograms, and saves them to disk.\"\"\"\n    if not cfg.mp3_dir.exists(): raise FileNotFoundError(f\"MP3 source directory not found: {cfg.mp3_dir}\")\n\n    # Check if output dir exists, ask before overwriting (optional)\n    if cfg.preprocessed_dir.exists() and any(cfg.preprocessed_dir.iterdir()):\n        print(f\"Preprocessed data directory '{cfg.preprocessed_dir}' already exists and is not empty.\")\n        # Decide whether to skip, overwrite, or exit\n        # For simplicity, we'll just proceed and potentially overwrite/add files.\n        # user_input = input(\"Directory not empty. Overwrite/Add files? (y/n): \").lower()\n        # if user_input != 'y': print(\"Exiting.\"); return\n        print(\"Directory not empty. Proceeding (may overwrite/add files)...\")\n    else:\n        print(f\"Creating preprocessed data directory: {cfg.preprocessed_dir}\")\n        cfg.preprocessed_dir.mkdir(parents=True, exist_ok=True)\n\n    mp3_files = list(cfg.mp3_dir.glob('*.mp3')) + list(cfg.mp3_dir.glob('*.MP3'))\n    if not mp3_files: raise FileNotFoundError(f\"No MP3 files found in {cfg.mp3_dir}\")\n    print(f\"Found {len(mp3_files)} MP3 files. Starting preprocessing...\")\n\n    # Use CPU for spectrogram conversion during preprocessing\n    preproc_device = \"cpu\"\n    spectrogram_converter = SpectrogramConverter(cfg, device=preproc_device)\n    target_sr = cfg.target_sample_rate\n    chunk_len_samples = cfg.chunk_length_samples\n    expected_time_steps = cfg.num_time_steps_per_chunk\n    processed_chunks, skipped_files = 0, 0\n\n    for mp3_path in tqdm(mp3_files, desc=\"Preprocessing MP3s\"):\n        try:\n            audio = pydub.AudioSegment.from_mp3(mp3_path)\n            if audio.frame_rate != target_sr: audio = audio.set_frame_rate(target_sr)\n            if audio.channels > 1: audio = audio.set_channels(1)\n            samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n            max_val = np.iinfo(np.int16).max; samples /= max_val\n            num_chunks = len(samples) // chunk_len_samples\n\n            for i in range(num_chunks):\n                start_idx = i * chunk_len_samples; end_idx = start_idx + chunk_len_samples\n                chunk_waveform = samples[start_idx:end_idx]\n                waveform_tensor = torch.from_numpy(chunk_waveform).unsqueeze(0).unsqueeze(0) # [1, 1, N]\n                spec_tensor = spectrogram_converter.spectrogram_from_waveform_tensor(waveform_tensor.to(preproc_device)) # [1, 1, F, T]\n\n                if spec_tensor.ndim != 4 or spec_tensor.shape[0] != 1 or spec_tensor.shape[1] != 1:\n                     print(f\"\\nWarning: Bad spec shape {spec_tensor.shape} for {mp3_path.name} chunk {i}. Skip.\"); continue\n                spec_tensor_squeezed = spec_tensor.squeeze(0).squeeze(0) # [F, T]\n                actual_time_steps = spec_tensor_squeezed.shape[1]\n\n                if actual_time_steps != expected_time_steps:\n                    if actual_time_steps < expected_time_steps:\n                        pad_width = expected_time_steps - actual_time_steps\n                        spec_tensor_squeezed = F.pad(spec_tensor_squeezed, (0, pad_width), mode='constant', value=0)\n                    else:\n                        spec_tensor_squeezed = spec_tensor_squeezed[:, :expected_time_steps]\n                    if spec_tensor_squeezed.shape[1] != expected_time_steps:\n                         print(f\"\\nWarning: Time step fix failed ({spec_tensor_squeezed.shape[1]} vs {expected_time_steps}) for {mp3_path.name} chunk {i}. Skip.\"); continue\n\n                output_filename = cfg.preprocessed_dir / f\"{mp3_path.stem}_chunk_{i:04d}.pt\"\n                # Save as CPU tensor using default pickle protocol\n                torch.save(spec_tensor_squeezed.cpu(), output_filename)\n                processed_chunks += 1\n\n        except pydub.exceptions.CouldntDecodeError: print(f\"\\nWarning: Could not decode {mp3_path}. Skipping.\"); skipped_files += 1\n        except FileNotFoundError: print(f\"\\nWarning: File disappeared {mp3_path}. Skipping.\"); skipped_files += 1\n        except Exception as e: print(f\"\\nError processing {mp3_path}: {e}\"); traceback.print_exc(); skipped_files += 1\n\n    print(f\"\\n--- Preprocessing Finished ---\")\n    print(f\"  Total chunks processed and saved: {processed_chunks}\")\n    print(f\"  Files skipped due to errors: {skipped_files}\")\n    if processed_chunks == 0: print(\"Warning: No chunks were processed. Check MP3 files and parameters.\")\n\n# --- 5. Main Execution ---\nif __name__ == \"__main__\":\n    print(\"\\nStarting preprocessing script...\")\n    try:\n        preprocess_mp3s(cfg)\n    except Exception as e:\n        print(f\"\\nAn error occurred during preprocessing: {e}\")\n        traceback.print_exc()\n    print(\"\\nPreprocessing script execution complete.\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #     VQ-VAE - SCRIPT 2: TRAINING LOOP      #\n# # (Loads .pt, Loads Checkpoint, Trains Model) #\n# ##############################################\n\n# --- 1. Imports ---\nprint(\"Importing libraries for training...\")\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import GradScaler, autocast # Use updated syntax\nfrom torch.nn.utils import clip_grad_norm_\n\nimport torchaudio\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass, field\nimport typing as T\nimport numpy as np\nimport io\nimport os\nimport glob\nimport time\nimport random\nimport warnings\nimport traceback\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport pydub # For audio recon during visualization\nfrom scipy.io import wavfile\n\n# --- 2. Configuration ---\n# !! IMPORTANT !! Ensure these parameters EXACTLY match the preprocessing script !!\n# !! AND reflect the desired state for this training run (e.g., mixed precision) !!\nprint(\"Setting up configuration...\")\n\n@dataclass\nclass Config:\n    # --- Paths ---\n    mp3_dir: Path = Path(\"/kaggle/input/demoscene-mp3s/mp3 2/\") # Kept for consistency if needed elsewhere\n    # !! Crucial: Path to EXISTING preprocessed spectrograms !!\n    preprocessed_dir: Path = Path(\"/kaggle/working/preprocessed_spectrograms/\")\n    # !! Crucial: Path to checkpoint directory !!\n    checkpoint_dir: Path = Path(\"/kaggle/working/vqvae_checkpoints/\")\n    # Optional: Specify a specific checkpoint to load, otherwise loads latest\n    checkpoint_load_path: T.Optional[Path] = None\n\n    # --- Preprocessing & Spectrogram Parameters (Must Match Preprocessing Run) ---\n    target_sample_rate: int = 22050\n    chunk_duration_ms: int = 2000\n    step_size_ms: int = 20\n    window_duration_ms: int = 50\n    padded_duration_ms: int = 200\n    num_frequencies: int = 128\n    min_frequency: int = 30\n    max_frequency: int = 8000\n    mel_scale_norm: T.Optional[str] = None\n    mel_scale_type: str = \"htk\"\n    power_for_spectrogram: float = 2.0\n    num_griffin_lim_iters: int = 32 # For visualization\n\n    # --- VQ-VAE Model Parameters (Must Match Previous Run if Resuming) ---\n    embedding_dim: int = 64\n    num_embeddings: int = 512\n    num_hiddens: int = 128\n    num_residual_layers: int = 2\n    num_residual_hiddens: int = 32\n    commitment_cost: float = 0.25\n\n    # --- Training Parameters ---\n    num_epochs: int = 25 # Total desired epochs\n    batch_size: int = 256 # Increased based on last run\n    learning_rate: float = 2e-4\n    # !! Set based on desired mode for THIS run !!\n    use_mixed_precision: bool = False # Set to False based on last stable run\n    gradient_clip_val: float = 1.0 # Max norm for gradients\n\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # Consider increasing num_workers if GPU util is low and CPU is bottleneck\n    num_workers: int = 0\n    pin_memory: bool = False # Set to True only if num_workers > 0\n\n    # --- Logging/Checkpointing ---\n    print_every_n_batches: int = 50\n    save_every_n_epochs: int = 1\n\n    # --- Derived Parameters ---\n    chunk_length_samples: int = field(init=False)\n    n_fft: int = field(init=False)\n    win_length: int = field(init=False)\n    hop_length: int = field(init=False)\n    num_time_steps_per_chunk: int = field(init=False)\n\n    def __post_init__(self):\n        # (Calculations remain the same)\n        self.chunk_length_samples = int(self.chunk_duration_ms / 1000.0 * self.target_sample_rate)\n        self.n_fft = int(self.padded_duration_ms / 1000.0 * self.target_sample_rate)\n        self.win_length = int(self.window_duration_ms / 1000.0 * self.target_sample_rate)\n        self.hop_length = int(self.step_size_ms / 1000.0 * self.target_sample_rate)\n        self.num_time_steps_per_chunk = (self.chunk_length_samples // self.hop_length) + 1\n        print(f\"Derived Config: Chunk Samples={self.chunk_length_samples}, n_fft={self.n_fft}, hop={self.hop_length}, win={self.win_length}, T={self.num_time_steps_per_chunk}\")\n        print(f\"Using Device: {self.device.upper()}\")\n        print(f\"Using Mixed Precision: {self.use_mixed_precision}\")\n        print(f\"DataLoader Workers: {self.num_workers}\")\n        print(f\"Batch Size: {self.batch_size}\")\n        print(f\"Gradient Clipping Norm: {self.gradient_clip_val}\")\n\n# Instantiate Configuration\ncfg = Config()\n\n# --- 3. Utility Functions & Classes ---\n# (Define audio_from_waveform and SpectrogramConverter EXACTLY as before)\nprint(\"Defining utilities...\")\n# ... (paste SpectrogramConverter and audio_from_waveform definitions here) ...\ndef audio_from_waveform(samples: np.ndarray, sample_rate: int, normalize: bool = False) -> pydub.AudioSegment:\n    if samples.ndim == 1: samples = samples[np.newaxis, :]\n    if samples.shape[0] != 1: samples = samples[0:1, :]\n    if normalize:\n        max_val = np.max(np.abs(samples));\n        if max_val > 1e-6: samples = samples / max_val * 0.95\n    samples_int16 = (samples * np.iinfo(np.int16).max).astype(np.int16)\n    samples_int16_transposed = samples_int16.transpose()\n    wav_bytes = io.BytesIO(); wavfile.write(wav_bytes, sample_rate, samples_int16_transposed); wav_bytes.seek(0)\n    return pydub.AudioSegment.from_wav(wav_bytes)\n\nclass SpectrogramConverter:\n    def __init__(self, cfg: Config, device: str):\n        self.cfg = cfg; self.device = torch.device(device)\n        print(f\"SpectrogramConverter using device: {self.device}\")\n        self.spectrogram_func = torchaudio.transforms.Spectrogram(n_fft=cfg.n_fft, hop_length=cfg.hop_length, win_length=cfg.win_length, power=cfg.power_for_spectrogram, center=True, pad_mode=\"reflect\", onesided=True, window_fn=torch.hann_window).to(self.device)\n        self.mel_scaler = torchaudio.transforms.MelScale(n_mels=cfg.num_frequencies, sample_rate=cfg.target_sample_rate, f_min=cfg.min_frequency, f_max=cfg.max_frequency, n_stft=cfg.n_fft // 2 + 1, norm=cfg.mel_scale_norm, mel_scale=cfg.mel_scale_type).to(self.device)\n        self.inverse_mel_scaler = torchaudio.transforms.InverseMelScale(n_stft=cfg.n_fft // 2 + 1, n_mels=cfg.num_frequencies, sample_rate=cfg.target_sample_rate, f_min=cfg.min_frequency, f_max=cfg.max_frequency, norm=cfg.mel_scale_norm, mel_scale=cfg.mel_scale_type).to(self.device)\n        self.db_scaler = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80.0).to(self.device)\n        self.min_db_level = -80.0\n    def _normalize_spectrogram(self, mel_spec_db: torch.Tensor) -> torch.Tensor:\n        mel_spec_db_clamped = torch.clamp(mel_spec_db, min=self.min_db_level); normalized = (mel_spec_db_clamped - self.min_db_level) / (-self.min_db_level); return torch.clamp(normalized, 0.0, 1.0)\n    def _denormalize_spectrogram(self, mel_spec_normalized: torch.Tensor) -> torch.Tensor:\n        mel_spec_db = (mel_spec_normalized * (-self.min_db_level)) + self.min_db_level; return mel_spec_db\n    def spectrogram_from_waveform_tensor(self, waveform_tensor: torch.Tensor) -> torch.Tensor:\n        if waveform_tensor.device != self.device: waveform_tensor = waveform_tensor.to(self.device)\n        with torch.no_grad(): spectrogram_pow = self.spectrogram_func(waveform_tensor); mel_spec_pow = self.mel_scaler(spectrogram_pow); mel_spec_db = self.db_scaler(mel_spec_pow); mel_spec_db_normalized = self._normalize_spectrogram(mel_spec_db)\n        return mel_spec_db_normalized\n    def audio_segment_from_spectrogram_tensor(self, mel_spec_db_normalized: torch.Tensor, target_len: T.Optional[int] = None) -> pydub.AudioSegment:\n        if mel_spec_db_normalized.device != self.device: mel_spec_db_normalized = mel_spec_db_normalized.to(self.device)\n        with torch.no_grad():\n            mel_spec_db = self._denormalize_spectrogram(mel_spec_db_normalized); mel_spec_power = torch.pow(10.0, mel_spec_db / 10.0); linear_spec_power = self.inverse_mel_scaler(mel_spec_power); linear_spec_amp = torch.sqrt(torch.clamp(linear_spec_power, min=1e-10))\n            griffin_lim_transform = torchaudio.transforms.GriffinLim(n_fft=self.cfg.n_fft, win_length=self.cfg.win_length, hop_length=self.cfg.hop_length, window_fn=torch.hann_window, power=2.0, n_iter=self.cfg.num_griffin_lim_iters, momentum=0.99, length=target_len, rand_init=True).to(self.device)\n            waveform = griffin_lim_transform(linear_spec_amp)\n        waveform_np = waveform.cpu().numpy(); return audio_from_waveform(samples=waveform_np, sample_rate=self.cfg.target_sample_rate, normalize=True)\n\n\n# --- 4. Preprocessing Function ---\nprint(\"Skipping Preprocessing step (assuming data exists).\")\n\n# --- 5. Optimized Dataset Class ---\n# (Define PreprocessedSpectrogramDataset EXACTLY as before, using weights_only=True)\nprint(\"Defining Optimized Dataset...\")\n# ... (paste PreprocessedSpectrogramDataset definition here) ...\nclass PreprocessedSpectrogramDataset(Dataset):\n    def __init__(self, preprocessed_dir: Path, expected_shape: T.Tuple[int, int]):\n        self.preprocessed_dir = preprocessed_dir; self.expected_shape = expected_shape\n        if not self.preprocessed_dir.exists(): raise FileNotFoundError(f\"Preprocessed data directory not found: {preprocessed_dir}. Cannot resume.\")\n        self.file_paths = sorted(list(preprocessed_dir.glob('*.pt')))\n        if not self.file_paths: raise FileNotFoundError(f\"No preprocessed .pt files found in {preprocessed_dir}. Cannot resume.\")\n        print(f\"Found {len(self.file_paths)} preprocessed spectrogram chunks in {preprocessed_dir}.\")\n        self._verify_first_item()\n    def _verify_first_item(self):\n        try: first_spec = torch.load(self.file_paths[0], map_location='cpu', weights_only=True)\n        except Exception as e: warnings.warn(f\"Could not load or verify first item {self.file_paths[0]}: {e}\")\n    def __len__(self): return len(self.file_paths)\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        try:\n            spectrogram = torch.load(file_path, map_location='cpu', weights_only=True)\n            if spectrogram.ndim != 2 or spectrogram.shape != self.expected_shape: return torch.zeros((1, *self.expected_shape), dtype=torch.float32)\n            return spectrogram.unsqueeze(0)\n        except Exception as e: print(f\"\\nError loading {file_path}: {e}. Returning zeros.\"); return torch.zeros((1, *self.expected_shape), dtype=torch.float32)\n\n\n# --- 6. VQ-VAE Model Definition ---\n# (Define VectorQuantizer, ResidualBlock, ResidualStack, Encoder, Decoder, VQVAE EXACTLY as before, with inplace=False fixes)\nprint(\"Defining VQ-VAE Model...\")\n# ... (paste VQ-VAE model definitions here) ...\nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings: int, embedding_dim: int, commitment_cost: float, decay: float = 0.99, epsilon: float = 1e-5):\n        super(VectorQuantizer, self).__init__(); self._embedding_dim = embedding_dim; self._num_embeddings = num_embeddings; self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim); self._embedding.weight.data.normal_(); self._commitment_cost = commitment_cost; self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings)); self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim)); self._ema_w.data.normal_(); self._decay = decay; self._epsilon = epsilon\n    def forward(self, inputs: torch.Tensor) -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        inputs_permuted = inputs.permute(0, 2, 3, 1).contiguous(); input_shape = inputs_permuted.shape; flat_input = inputs_permuted.view(-1, self._embedding_dim); distances = (torch.sum(flat_input**2, dim=1, keepdim=True) + torch.sum(self._embedding.weight**2, dim=1) - 2 * torch.matmul(flat_input, self._embedding.weight.t())); encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1); encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device).scatter_(1, encoding_indices, 1); quantized_flat = torch.matmul(encodings, self._embedding.weight); quantized_permuted = quantized_flat.view(input_shape)\n        if self.training:\n            with torch.no_grad(): self._ema_cluster_size = self._ema_cluster_size * self._decay + (1 - self._decay) * torch.sum(encodings, 0); dw = torch.matmul(encodings.t(), flat_input); self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw); n = torch.sum(self._ema_cluster_size); self._ema_cluster_size = ((self._ema_cluster_size + self._epsilon) / (n + self._num_embeddings * self._epsilon) * n); embed_normalized = self._ema_w / self._ema_cluster_size.unsqueeze(1); self._embedding.weight.data.copy_(embed_normalized)\n        e_latent_loss = F.mse_loss(quantized_permuted.detach(), inputs_permuted); q_latent_loss = F.mse_loss(quantized_permuted, inputs_permuted.detach()); vq_loss = q_latent_loss + self._commitment_cost * e_latent_loss; quantized_permuted_sg = inputs_permuted + (quantized_permuted - inputs_permuted).detach(); avg_probs = torch.mean(encodings, dim=0); perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10))); quantized_final = quantized_permuted_sg.permute(0, 3, 1, 2).contiguous(); return vq_loss, quantized_final, perplexity, encodings\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, out_channels: int): super(ResidualBlock, self).__init__(); self._block = nn.Sequential(nn.ReLU(), nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(), nn.Conv2d(in_channels=num_hiddens, out_channels=out_channels, kernel_size=1, stride=1, bias=False))\n    def forward(self, x: torch.Tensor) -> torch.Tensor: return x + self._block(x)\nclass ResidualStack(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, out_channels: int): super(ResidualStack, self).__init__(); self._num_residual_layers = num_residual_layers; self._layers = nn.ModuleList([ResidualBlock(in_channels, num_hiddens, out_channels) for _ in range(num_residual_layers)])\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for layer in self._layers: x = layer(x)\n        return F.relu(x)\nclass Encoder(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, num_residual_hiddens: int): super(Encoder, self).__init__(); self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens // 2, kernel_size=4, stride=2, padding=1); self._conv_2 = nn.Conv2d(in_channels=num_hiddens // 2, out_channels=num_hiddens, kernel_size=4, stride=2, padding=1); self._conv_3 = nn.Conv2d(in_channels=num_hiddens, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1); self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_residual_hiddens, num_residual_layers=num_residual_layers, out_channels=num_hiddens)\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor: x = F.relu(self._conv_1(inputs)); x = F.relu(self._conv_2(x)); x = F.relu(self._conv_3(x)); x = self._residual_stack(x); return x\nclass Decoder(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, num_residual_hiddens: int, out_channels: int): super(Decoder, self).__init__(); self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1); self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_residual_hiddens, num_residual_layers=num_residual_layers, out_channels=num_hiddens); self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, out_channels=num_hiddens // 2, kernel_size=4, stride=2, padding=1); self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens // 2, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor: x = F.relu(self._conv_1(inputs)); x = self._residual_stack(x); x = F.relu(self._conv_trans_1(x)); x_recon = self._conv_trans_2(x); return x_recon\nclass VQVAE(nn.Module):\n    def __init__(self, cfg: Config): super(VQVAE, self).__init__(); self.cfg = cfg; self._encoder = Encoder(in_channels=1, num_hiddens=cfg.num_hiddens, num_residual_layers=cfg.num_residual_layers, num_residual_hiddens=cfg.num_residual_hiddens); self._pre_vq_conv = nn.Conv2d(in_channels=cfg.num_hiddens, out_channels=cfg.embedding_dim, kernel_size=1, stride=1); self._vq_layer = VectorQuantizer(num_embeddings=cfg.num_embeddings, embedding_dim=cfg.embedding_dim, commitment_cost=cfg.commitment_cost); self._decoder = Decoder(in_channels=cfg.embedding_dim, num_hiddens=cfg.num_hiddens, num_residual_layers=cfg.num_residual_layers, num_residual_hiddens=cfg.num_residual_hiddens, out_channels=1)\n    def forward(self, x: torch.Tensor) -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: z_e = self._encoder(x); z_e_conv = self._pre_vq_conv(z_e); vq_loss, z_q, perplexity, _ = self._vq_layer(z_e_conv); x_recon = self._decoder(z_q); return vq_loss, x_recon, perplexity\n\n\n# --- 7. Training Setup ---\nprint(\"Setting up training components...\")\n\n# --- Initialize Dataset and DataLoader ---\nprint(\"Initializing Dataset and DataLoader from preprocessed data...\")\ntry:\n    expected_spec_shape = (cfg.num_frequencies, cfg.num_time_steps_per_chunk)\n    dataset = PreprocessedSpectrogramDataset(cfg.preprocessed_dir, expected_shape=expected_spec_shape)\n    dataloader = DataLoader(\n        dataset, batch_size=cfg.batch_size, shuffle=True,\n        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=True\n    )\n    print(f\"DataLoader created with {len(dataloader)} batches per epoch.\")\nexcept FileNotFoundError as e: print(f\"FATAL ERROR: {e}\"); raise e\nexcept Exception as e: print(f\"FATAL ERROR initializing dataset/dataloader: {e}\"); traceback.print_exc(); raise e\n\n# --- Initialize Model, Optimizer, Scaler ---\nprint(\"Initializing Model, Optimizer, and Scaler...\")\nmodel = VQVAE(cfg).to(cfg.device)\noptimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, amsgrad=False)\nscaler = torch.amp.GradScaler(cfg.device, enabled=cfg.use_mixed_precision)\nprint(f\"Model created on {cfg.device}. Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# --- Checkpoint Loading Logic ---\nstart_epoch = 0\ntrain_res_recon_error, train_res_perplexity, train_res_vq_loss = [], [], []\ncfg.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n# Determine the checkpoint to load\nload_path = cfg.checkpoint_load_path # Use specified path if provided\n\nif not load_path: # Otherwise, find the latest checkpoint in the directory\n    checkpoint_files = list(cfg.checkpoint_dir.glob(\"vqvae_checkpoint_epoch_*.pth\")) # Get all matching files\n\n    if checkpoint_files:\n        try:\n            # Define a function to extract the epoch number as an integer\n            def get_epoch_num(path):\n                # Extracts the number after the last '_' and before '.pth'\n                return int(path.stem.split('_')[-1])\n\n            # Sort the files based on the extracted epoch number, descending\n            checkpoint_files.sort(key=get_epoch_num, reverse=True)\n\n            # The first file in the sorted list is now the numerically latest\n            load_path = checkpoint_files[0]\n            print(f\"Found latest checkpoint (numerical sort): {load_path}\")\n\n        except (ValueError, IndexError) as e:\n            # Handle cases where filenames might not match the expected pattern\n            print(f\"Warning: Could not parse epoch number from all checkpoint filenames ({e}). Falling back to default sort.\")\n            # Fallback to potentially incorrect lexicographical sort if parsing fails\n            checkpoint_files.sort(reverse=True)\n            if checkpoint_files:\n                 load_path = checkpoint_files[0]\n                 print(f\"Found latest checkpoint (default sort - may be incorrect): {load_path}\")\n            else:\n                 load_path = None # No valid files found even with fallback\n\n# --- Now, proceed with loading if a valid load_path was determined ---\nif load_path and load_path.exists():\n    print(f\"Loading checkpoint: {load_path}\")\n    try:\n        checkpoint = torch.load(load_path, map_location=cfg.device) # weights_only=False needed\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if 'scaler_state_dict' in checkpoint and cfg.use_mixed_precision:\n             scaler.load_state_dict(checkpoint['scaler_state_dict'])\n             print(\"Loaded GradScaler state.\")\n        elif 'scaler_state_dict' in checkpoint and not cfg.use_mixed_precision:\n             print(\"Checkpoint contains scaler state, but mixed precision is disabled. Ignoring scaler state.\")\n        # Get epoch number saved *inside* the loaded checkpoint file\n        saved_epoch = checkpoint.get('epoch', -1)\n        start_epoch = saved_epoch + 1 # Start from the next epoch\n        train_res_recon_error = checkpoint.get('train_res_recon_error', [])\n        train_res_perplexity = checkpoint.get('train_res_perplexity', [])\n        train_res_vq_loss = checkpoint.get('train_res_vq_loss', [])\n        print(f\"Checkpoint loaded successfully. Resuming training from epoch {start_epoch}.\")\n    except Exception as e:\n        print(f\"Error loading checkpoint {load_path}: {e}. Starting fresh from epoch 0.\")\n        start_epoch = 0; train_res_recon_error, train_res_perplexity, train_res_vq_loss = [], [], []\n        load_path = None # Ensure we don't try to use a failed load_path later\nelse:\n    if cfg.checkpoint_load_path: print(f\"Specified checkpoint not found: {cfg.checkpoint_load_path}. Starting fresh.\")\n    else: print(f\"No valid checkpoint found in {cfg.checkpoint_dir}. Starting fresh from epoch 0.\")\n\n# --- The rest of the script (Training Loop, Visualization, etc.) remains the same ---\n# ... (paste the rest of Script 2 here) ...\n# --- 8. Training Loop ---\nprint(f\"\\n--- Starting Training from Epoch {start_epoch} / {cfg.num_epochs} ---\")\n\nfor epoch in range(start_epoch, cfg.num_epochs):\n    model.train()\n    epoch_recon_loss, epoch_vq_loss, epoch_perplexity, epoch_total_loss = 0.0, 0.0, 0.0, 0.0\n    batches_processed, load_errors = 0, 0\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n\n    for batch_idx, data in pbar:\n        if data.abs().sum() < 1e-9: load_errors += 1; continue\n        data = data.to(cfg.device)\n        optimizer.zero_grad(set_to_none=True)\n        try:\n            with torch.amp.autocast(device_type=cfg.device, dtype=torch.float16, enabled=cfg.use_mixed_precision):\n                vq_loss, data_recon, perplexity = model(data)\n                if data_recon.shape != data.shape:\n                    min_t = min(data_recon.shape[-1], data.shape[-1])\n                    if data.shape[-1] > min_t: data_for_loss, recon_for_loss = data[..., :min_t], data_recon\n                    else: data_for_loss, recon_for_loss = data, data_recon[..., :min_t]\n                    if batch_idx == 0: print(f\"\\nWarning: Shape mismatch! Recon: {data_recon.shape}, Data: {data.shape}. Loss on T={min_t}\")\n                else:\n                    data_for_loss, recon_for_loss = data, data_recon\n                recon_error = F.mse_loss(recon_for_loss, data_for_loss)\n                loss = recon_error + vq_loss\n\n            if cfg.use_mixed_precision:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_val)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_val)\n                optimizer.step()\n\n            epoch_recon_loss += recon_error.item(); epoch_vq_loss += vq_loss.item()\n            epoch_perplexity += perplexity.item(); epoch_total_loss += loss.item()\n            batches_processed += 1\n\n            log_dict = {'Loss': f'{loss.item():.4f}', 'Recon': f'{recon_error.item():.4f}', 'VQ': f'{vq_loss.item():.4f}', 'Perp': f'{perplexity.item():.2f}', 'Errors': load_errors}\n            if cfg.use_mixed_precision: log_dict['Scale'] = f'{scaler.get_scale():.1f}'\n            if (batch_idx + 1) % cfg.print_every_n_batches == 0 or batch_idx == len(dataloader) - 1:\n                pbar.set_postfix(log_dict)\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower(): print(f\"\\nOOM Error @ batch {batch_idx}. Skipping batch.\"); torch.cuda.empty_cache(); continue\n            else: print(f\"\\nRuntimeError @ batch {batch_idx}: {e}\"); traceback.print_exc(); continue\n        except Exception as e: print(f\"\\nError @ batch {batch_idx}: {e}\"); traceback.print_exc(); continue\n\n    # --- End of Epoch Summary ---\n    if batches_processed > 0:\n        avg_recon_loss = epoch_recon_loss / batches_processed; avg_vq_loss = epoch_vq_loss / batches_processed\n        avg_perplexity = epoch_perplexity / batches_processed; avg_total_loss = epoch_total_loss / batches_processed\n        train_res_recon_error.append(avg_recon_loss); train_res_perplexity.append(avg_perplexity); train_res_vq_loss.append(avg_vq_loss)\n        print(f\"\\nEpoch [{epoch+1}/{cfg.num_epochs}] Summary:\")\n        print(f\"  Avg Total Loss: {avg_total_loss:.4f} | Avg Recon Error: {avg_recon_loss:.4f} | Avg VQ Loss: {avg_vq_loss:.4f} | Avg Perp: {avg_perplexity:.2f}\")\n        if load_errors > 0: print(f\"  Skipped Batches (Load Errors): {load_errors}\")\n    else:\n        print(f\"\\nEpoch [{epoch+1}/{cfg.num_epochs}] - No batches processed.\"); train_res_recon_error.append(float('nan')); train_res_perplexity.append(float('nan')); train_res_vq_loss.append(float('nan'))\n\n    # --- Checkpoint Saving ---\n    if (epoch + 1) % cfg.save_every_n_epochs == 0 or epoch == cfg.num_epochs - 1:\n        checkpoint_filename = f\"vqvae_checkpoint_epoch_{epoch+1}.pth\"\n        checkpoint_full_path = cfg.checkpoint_dir / checkpoint_filename\n        print(f\"Saving checkpoint: {checkpoint_full_path} ...\")\n        try:\n            save_dict = {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n                         'train_res_recon_error': train_res_recon_error, 'train_res_perplexity': train_res_perplexity, 'train_res_vq_loss': train_res_vq_loss, 'config': cfg}\n            if cfg.use_mixed_precision: save_dict['scaler_state_dict'] = scaler.state_dict()\n            torch.save(save_dict, checkpoint_full_path)\n            print(\"Checkpoint saved.\")\n        except Exception as cs_e: print(f\"Error saving checkpoint: {cs_e}\")\n\nprint(\"\\n--- Training Finished ---\")\n\n# --- 9. Visualization / Evaluation (Optional) ---\nprint(\"\\nVisualizing one reconstruction example...\")\n# ... (paste visualization code here) ...\nmodel.eval()\ntry:\n    vis_converter = SpectrogramConverter(cfg, device=cfg.device)\n    vis_dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n    original_data_batch = next(iter(vis_dataloader)).to(cfg.device)\n    if original_data_batch.abs().sum() < 1e-9: print(\"Loaded empty batch for viz. Skipping.\")\n    elif original_data_batch.ndim != 4 or original_data_batch.shape[1] != 1: print(f\"Unexpected viz batch shape: {original_data_batch.shape}. Skipping.\")\n    else:\n        with torch.no_grad(): vq_loss, data_recon_batch, perplexity = model(original_data_batch)\n        original_spec_plot = original_data_batch[0, 0].cpu().numpy(); reconstructed_spec_plot = data_recon_batch[0, 0].cpu().numpy()\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5)); fig.suptitle(f\"Example Reconstruction - Epoch {epoch+1}\", fontsize=16)\n        im_orig = axes[0].imshow(original_spec_plot, aspect='auto', origin='lower', cmap='viridis'); axes[0].set_title(\"Original Spectrogram (Normalized)\"); axes[0].set_xlabel(\"Time Steps\"); axes[0].set_ylabel(\"Mel Bins\"); fig.colorbar(im_orig, ax=axes[0], shrink=0.6, label=\"Normalized Amplitude\")\n        im_recon = axes[1].imshow(reconstructed_spec_plot, aspect='auto', origin='lower', cmap='viridis'); axes[1].set_title(\"Reconstructed Spectrogram (Normalized)\"); axes[1].set_xlabel(\"Time Steps\"); fig.colorbar(im_recon, ax=axes[1], shrink=0.6, label=\"Normalized Amplitude\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n        print(\"\\nAttempting audio reconstruction...\")\n        try:\n            estimated_recon_samples = original_spec_plot.shape[1] * cfg.hop_length\n            print(\"Reconstructing Original Audio...\"); original_audio = vis_converter.audio_segment_from_spectrogram_tensor(original_data_batch[0], target_len=estimated_recon_samples); print(\"Original Audio (Reconstructed):\")\n            print(\"\\nReconstructing VQ-VAE Output Audio...\"); reconstructed_audio = vis_converter.audio_segment_from_spectrogram_tensor(data_recon_batch[0], target_len=estimated_recon_samples); print(\"Reconstructed Audio (From Model):\")\n        except ImportError: print(\"Install 'ffmpeg' or 'libav' for audio export/display.\")\n        except Exception as audio_err: print(f\"Error during audio recon: {audio_err}\"); traceback.print_exc()\nexcept StopIteration: print(\"Could not get viz batch (Dataset empty?).\")\nexcept Exception as viz_err: print(f\"\\nError during visualization: {viz_err}\"); traceback.print_exc()\n\n\n# --- 10. Plot Loss Curves (Optional) ---\nprint(\"\\nPlotting training curves...\")\n# ... (paste plotting code here) ...\ntry:\n    epochs_ran = list(range(len(train_res_recon_error)))\n    if epochs_ran:\n        plt.figure(figsize=(12, 8))\n        plt.subplot(3, 1, 1); plt.plot(epochs_ran, train_res_recon_error, marker='o'); plt.title('Reconstruction Error (MSE Loss)'); plt.ylabel('MSE Loss'); plt.grid(True)\n        plt.subplot(3, 1, 2); plt.plot(epochs_ran, train_res_vq_loss, marker='o'); plt.title('VQ Loss (Codebook + Commitment)'); plt.ylabel('VQ Loss'); plt.grid(True)\n        plt.subplot(3, 1, 3); plt.plot(epochs_ran, train_res_perplexity, marker='o'); plt.title('Codebook Perplexity'); plt.xlabel('Epoch'); plt.ylabel('Perplexity'); plt.grid(True)\n        plt.tight_layout(); plt.show()\n    else: print(\"No training data to plot.\")\nexcept Exception as plot_err: print(f\"Error plotting loss curves: {plot_err}\")\n\n\nprint(\"\\n--- Training Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T02:10:11.890589Z","iopub.execute_input":"2025-04-23T02:10:11.891156Z"}},"outputs":[{"name":"stdout","text":"Importing libraries for training...\nSetting up configuration...\nDerived Config: Chunk Samples=44100, n_fft=4410, hop=441, win=1102, T=101\nUsing Device: CUDA\nUsing Mixed Precision: False\nDataLoader Workers: 0\nBatch Size: 256\nGradient Clipping Norm: 1.0\nDefining utilities...\nSkipping Preprocessing step (assuming data exists).\nDefining Optimized Dataset...\nDefining VQ-VAE Model...\nSetting up training components...\nInitializing Dataset and DataLoader from preprocessed data...\nFound 61880 preprocessed spectrogram chunks in /kaggle/working/preprocessed_spectrograms.\nDataLoader created with 241 batches per epoch.\nInitializing Model, Optimizer, and Scaler...\nModel created on cuda. Parameters: 723,521\nFound latest checkpoint (numerical sort): /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_13.pth\nLoading checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_13.pth\nCheckpoint loaded successfully. Resuming training from epoch 13.\n\n--- Starting Training from Epoch 13 / 25 ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2433190335.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(load_path, map_location=cfg.device) # weights_only=False needed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8e3ea0acd84143acbe4347ac19c53d"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [14/25] Summary:\n  Avg Total Loss: 0.0021 | Avg Recon Error: 0.0009 | Avg VQ Loss: 0.0012 | Avg Perp: 4.25\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_14.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a32761a734941eaa0c0275d08b6a561"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [15/25] Summary:\n  Avg Total Loss: 0.0020 | Avg Recon Error: 0.0009 | Avg VQ Loss: 0.0012 | Avg Perp: 4.56\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_15.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b920cb7f2efa430389db3929ba208272"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [16/25] Summary:\n  Avg Total Loss: 0.0019 | Avg Recon Error: 0.0008 | Avg VQ Loss: 0.0011 | Avg Perp: 4.80\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_16.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e1bc3a798143fd9aecea12e740c071"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [17/25] Summary:\n  Avg Total Loss: 0.0018 | Avg Recon Error: 0.0008 | Avg VQ Loss: 0.0010 | Avg Perp: 5.03\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_17.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b287952d2245aa89cf609a4e2183ef"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"il prossimo rerunna tutto dall'epoca 0","metadata":{}},{"cell_type":"code","source":"# # VQ-VAE Training Script - RESTART FROM EPOCH 0 #\n# # (Skips Preprocessing, Disables AMP, No Checkpoint Load) #\n# ##############################################\n\n# --- 1. Imports ---\nprint(\"Importing libraries...\")\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n# Use updated torch.amp syntax BUT will be disabled by config\nfrom torch.amp import GradScaler, autocast\n# Import for gradient clipping\nfrom torch.nn.utils import clip_grad_norm_\n\nimport torchaudio\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass, field\nimport typing as T\nimport numpy as np\nimport io\nimport os\nimport glob\nimport time\nimport random\nimport warnings\nimport traceback\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport pydub\nfrom scipy.io import wavfile\n\n# --- 2. Configuration ---\nprint(\"Setting up configuration...\")\n\n@dataclass\nclass Config:\n    # --- Paths ---\n    mp3_dir: Path = Path(\"/kaggle/input/demoscene-mp3s/mp3 2/\")\n    preprocessed_dir: Path = Path(\"/kaggle/working/preprocessed_spectrograms/\")\n    checkpoint_dir: Path = Path(\"/kaggle/working/vqvae_checkpoints/\")\n    # Force no checkpoint loading for restart\n    checkpoint_load_path: T.Optional[Path] = None\n\n    # --- Preprocessing & Spectrogram Parameters (Must Match Previous Run) ---\n    target_sample_rate: int = 22050\n    chunk_duration_ms: int = 2000\n    step_size_ms: int = 20\n    window_duration_ms: int = 50\n    padded_duration_ms: int = 200\n    num_frequencies: int = 128\n    min_frequency: int = 30\n    max_frequency: int = 8000\n    mel_scale_norm: T.Optional[str] = None\n    mel_scale_type: str = \"htk\"\n    power_for_spectrogram: float = 2.0\n    num_griffin_lim_iters: int = 32\n\n    # --- VQ-VAE Model Parameters (Must Match Previous Run) ---\n    embedding_dim: int = 64\n    num_embeddings: int = 512\n    num_hiddens: int = 128\n    num_residual_layers: int = 2\n    num_residual_hiddens: int = 32\n    commitment_cost: float = 0.25\n\n    # --- Training Parameters ---\n    num_epochs: int = 25\n    batch_size: int = 256\n    learning_rate: float = 2e-4 # Keep initial LR for now, reduce if still unstable\n    # !! DISABLE Mixed Precision !!\n    use_mixed_precision: bool = False\n    # Add Gradient Clipping value\n    gradient_clip_val: float = 1.0 # Max norm for gradients\n\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    num_workers: int = 0\n    pin_memory: bool = False\n\n    # --- Logging/Checkpointing ---\n    print_every_n_batches: int = 50\n    save_every_n_epochs: int = 1\n\n    # --- Derived Parameters ---\n    chunk_length_samples: int = field(init=False)\n    n_fft: int = field(init=False)\n    win_length: int = field(init=False)\n    hop_length: int = field(init=False)\n    num_time_steps_per_chunk: int = field(init=False)\n\n    def __post_init__(self):\n        # (Calculations remain the same)\n        self.chunk_length_samples = int(self.chunk_duration_ms / 1000.0 * self.target_sample_rate)\n        self.n_fft = int(self.padded_duration_ms / 1000.0 * self.target_sample_rate)\n        self.win_length = int(self.window_duration_ms / 1000.0 * self.target_sample_rate)\n        self.hop_length = int(self.step_size_ms / 1000.0 * self.target_sample_rate)\n        self.num_time_steps_per_chunk = (self.chunk_length_samples // self.hop_length) + 1\n        print(f\"Derived Config: Chunk Samples={self.chunk_length_samples}, n_fft={self.n_fft}, hop={self.hop_length}, win={self.win_length}, T={self.num_time_steps_per_chunk}\")\n        print(f\"Using Device: {self.device.upper()}\")\n        print(f\"!! Using Mixed Precision: {self.use_mixed_precision} !!\") # Highlight change\n        print(f\"DataLoader Workers: {self.num_workers}\")\n        print(f\"Batch Size: {self.batch_size}\")\n        print(f\"Gradient Clipping Norm: {self.gradient_clip_val}\")\n\n# Instantiate Configuration\ncfg = Config()\n\n# --- 3. Utility Functions & Classes ---\n# (Define audio_from_waveform and SpectrogramConverter EXACTLY as before)\nprint(\"Defining utilities...\")\n# ... (paste SpectrogramConverter and audio_from_waveform definitions here) ...\ndef audio_from_waveform(samples: np.ndarray, sample_rate: int, normalize: bool = False) -> pydub.AudioSegment:\n    if samples.ndim == 1: samples = samples[np.newaxis, :]\n    if samples.shape[0] != 1: samples = samples[0:1, :]\n    if normalize:\n        max_val = np.max(np.abs(samples))\n        if max_val > 1e-6: samples = samples / max_val * 0.95\n    samples_int16 = (samples * np.iinfo(np.int16).max).astype(np.int16)\n    samples_int16_transposed = samples_int16.transpose()\n    wav_bytes = io.BytesIO(); wavfile.write(wav_bytes, sample_rate, samples_int16_transposed); wav_bytes.seek(0)\n    return pydub.AudioSegment.from_wav(wav_bytes)\n\nclass SpectrogramConverter:\n    def __init__(self, cfg: Config, device: str):\n        self.cfg = cfg; self.device = torch.device(device)\n        print(f\"SpectrogramConverter using device: {self.device}\")\n        self.spectrogram_func = torchaudio.transforms.Spectrogram(n_fft=cfg.n_fft, hop_length=cfg.hop_length, win_length=cfg.win_length, power=cfg.power_for_spectrogram, center=True, pad_mode=\"reflect\", onesided=True, window_fn=torch.hann_window).to(self.device)\n        self.mel_scaler = torchaudio.transforms.MelScale(n_mels=cfg.num_frequencies, sample_rate=cfg.target_sample_rate, f_min=cfg.min_frequency, f_max=cfg.max_frequency, n_stft=cfg.n_fft // 2 + 1, norm=cfg.mel_scale_norm, mel_scale=cfg.mel_scale_type).to(self.device)\n        self.inverse_mel_scaler = torchaudio.transforms.InverseMelScale(n_stft=cfg.n_fft // 2 + 1, n_mels=cfg.num_frequencies, sample_rate=cfg.target_sample_rate, f_min=cfg.min_frequency, f_max=cfg.max_frequency, norm=cfg.mel_scale_norm, mel_scale=cfg.mel_scale_type).to(self.device)\n        self.db_scaler = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80.0).to(self.device)\n        self.min_db_level = -80.0\n    def _normalize_spectrogram(self, mel_spec_db: torch.Tensor) -> torch.Tensor:\n        mel_spec_db_clamped = torch.clamp(mel_spec_db, min=self.min_db_level); normalized = (mel_spec_db_clamped - self.min_db_level) / (-self.min_db_level); return torch.clamp(normalized, 0.0, 1.0)\n    def _denormalize_spectrogram(self, mel_spec_normalized: torch.Tensor) -> torch.Tensor:\n        mel_spec_db = (mel_spec_normalized * (-self.min_db_level)) + self.min_db_level; return mel_spec_db\n    def spectrogram_from_waveform_tensor(self, waveform_tensor: torch.Tensor) -> torch.Tensor:\n        if waveform_tensor.device != self.device: waveform_tensor = waveform_tensor.to(self.device)\n        with torch.no_grad(): spectrogram_pow = self.spectrogram_func(waveform_tensor); mel_spec_pow = self.mel_scaler(spectrogram_pow); mel_spec_db = self.db_scaler(mel_spec_pow); mel_spec_db_normalized = self._normalize_spectrogram(mel_spec_db)\n        return mel_spec_db_normalized\n    def audio_segment_from_spectrogram_tensor(self, mel_spec_db_normalized: torch.Tensor, target_len: T.Optional[int] = None) -> pydub.AudioSegment:\n        if mel_spec_db_normalized.device != self.device: mel_spec_db_normalized = mel_spec_db_normalized.to(self.device)\n        with torch.no_grad():\n            mel_spec_db = self._denormalize_spectrogram(mel_spec_db_normalized); mel_spec_power = torch.pow(10.0, mel_spec_db / 10.0); linear_spec_power = self.inverse_mel_scaler(mel_spec_power); linear_spec_amp = torch.sqrt(torch.clamp(linear_spec_power, min=1e-10))\n            griffin_lim_transform = torchaudio.transforms.GriffinLim(n_fft=self.cfg.n_fft, win_length=self.cfg.win_length, hop_length=self.cfg.hop_length, window_fn=torch.hann_window, power=2.0, n_iter=self.cfg.num_griffin_lim_iters, momentum=0.99, length=target_len, rand_init=True).to(self.device)\n            waveform = griffin_lim_transform(linear_spec_amp)\n        waveform_np = waveform.cpu().numpy(); return audio_from_waveform(samples=waveform_np, sample_rate=self.cfg.target_sample_rate, normalize=True)\n\n\n# --- 4. Preprocessing Function ---\nprint(\"Skipping Preprocessing step.\")\n\n# --- 5. Optimized Dataset Class ---\n# (Define PreprocessedSpectrogramDataset EXACTLY as before, using weights_only=True)\nprint(\"Defining Optimized Dataset...\")\n# ... (paste PreprocessedSpectrogramDataset definition here) ...\nclass PreprocessedSpectrogramDataset(Dataset):\n    def __init__(self, preprocessed_dir: Path, expected_shape: T.Tuple[int, int]):\n        self.preprocessed_dir = preprocessed_dir; self.expected_shape = expected_shape\n        if not self.preprocessed_dir.exists(): raise FileNotFoundError(f\"Preprocessed data directory not found: {preprocessed_dir}. Cannot resume.\")\n        self.file_paths = sorted(list(preprocessed_dir.glob('*.pt')))\n        if not self.file_paths: raise FileNotFoundError(f\"No preprocessed .pt files found in {preprocessed_dir}. Cannot resume.\")\n        print(f\"Found {len(self.file_paths)} preprocessed spectrogram chunks in {preprocessed_dir}.\")\n        self._verify_first_item()\n    def _verify_first_item(self):\n        try: first_spec = torch.load(self.file_paths[0], map_location='cpu', weights_only=True)\n        except Exception as e: warnings.warn(f\"Could not load or verify first item {self.file_paths[0]}: {e}\")\n    def __len__(self): return len(self.file_paths)\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        try:\n            spectrogram = torch.load(file_path, map_location='cpu', weights_only=True)\n            if spectrogram.ndim != 2 or spectrogram.shape != self.expected_shape: return torch.zeros((1, *self.expected_shape), dtype=torch.float32)\n            return spectrogram.unsqueeze(0)\n        except Exception as e: print(f\"\\nError loading {file_path}: {e}. Returning zeros.\"); return torch.zeros((1, *self.expected_shape), dtype=torch.float32)\n\n\n# --- 6. VQ-VAE Model Definition ---\n# (Define VectorQuantizer, ResidualBlock, ResidualStack, Encoder, Decoder, VQVAE EXACTLY as before, with inplace=False fixes)\nprint(\"Defining VQ-VAE Model...\")\n# ... (paste VQ-VAE model definitions here) ...\nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings: int, embedding_dim: int, commitment_cost: float, decay: float = 0.99, epsilon: float = 1e-5):\n        super(VectorQuantizer, self).__init__(); self._embedding_dim = embedding_dim; self._num_embeddings = num_embeddings; self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim); self._embedding.weight.data.normal_(); self._commitment_cost = commitment_cost; self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings)); self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim)); self._ema_w.data.normal_(); self._decay = decay; self._epsilon = epsilon\n    def forward(self, inputs: torch.Tensor) -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        inputs_permuted = inputs.permute(0, 2, 3, 1).contiguous(); input_shape = inputs_permuted.shape; flat_input = inputs_permuted.view(-1, self._embedding_dim); distances = (torch.sum(flat_input**2, dim=1, keepdim=True) + torch.sum(self._embedding.weight**2, dim=1) - 2 * torch.matmul(flat_input, self._embedding.weight.t())); encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1); encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device).scatter_(1, encoding_indices, 1); quantized_flat = torch.matmul(encodings, self._embedding.weight); quantized_permuted = quantized_flat.view(input_shape)\n        if self.training:\n            with torch.no_grad(): self._ema_cluster_size = self._ema_cluster_size * self._decay + (1 - self._decay) * torch.sum(encodings, 0); dw = torch.matmul(encodings.t(), flat_input); self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw); n = torch.sum(self._ema_cluster_size); self._ema_cluster_size = ((self._ema_cluster_size + self._epsilon) / (n + self._num_embeddings * self._epsilon) * n); embed_normalized = self._ema_w / self._ema_cluster_size.unsqueeze(1); self._embedding.weight.data.copy_(embed_normalized)\n        e_latent_loss = F.mse_loss(quantized_permuted.detach(), inputs_permuted); q_latent_loss = F.mse_loss(quantized_permuted, inputs_permuted.detach()); vq_loss = q_latent_loss + self._commitment_cost * e_latent_loss; quantized_permuted_sg = inputs_permuted + (quantized_permuted - inputs_permuted).detach(); avg_probs = torch.mean(encodings, dim=0); perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10))); quantized_final = quantized_permuted_sg.permute(0, 3, 1, 2).contiguous(); return vq_loss, quantized_final, perplexity, encodings\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, out_channels: int): super(ResidualBlock, self).__init__(); self._block = nn.Sequential(nn.ReLU(), nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(), nn.Conv2d(in_channels=num_hiddens, out_channels=out_channels, kernel_size=1, stride=1, bias=False))\n    def forward(self, x: torch.Tensor) -> torch.Tensor: return x + self._block(x)\nclass ResidualStack(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, out_channels: int): super(ResidualStack, self).__init__(); self._num_residual_layers = num_residual_layers; self._layers = nn.ModuleList([ResidualBlock(in_channels, num_hiddens, out_channels) for _ in range(num_residual_layers)])\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for layer in self._layers: x = layer(x)\n        return F.relu(x)\nclass Encoder(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, num_residual_hiddens: int): super(Encoder, self).__init__(); self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens // 2, kernel_size=4, stride=2, padding=1); self._conv_2 = nn.Conv2d(in_channels=num_hiddens // 2, out_channels=num_hiddens, kernel_size=4, stride=2, padding=1); self._conv_3 = nn.Conv2d(in_channels=num_hiddens, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1); self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_residual_hiddens, num_residual_layers=num_residual_layers, out_channels=num_hiddens)\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor: x = F.relu(self._conv_1(inputs)); x = F.relu(self._conv_2(x)); x = F.relu(self._conv_3(x)); x = self._residual_stack(x); return x\nclass Decoder(nn.Module):\n    def __init__(self, in_channels: int, num_hiddens: int, num_residual_layers: int, num_residual_hiddens: int, out_channels: int): super(Decoder, self).__init__(); self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1); self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_residual_hiddens, num_residual_layers=num_residual_layers, out_channels=num_hiddens); self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, out_channels=num_hiddens // 2, kernel_size=4, stride=2, padding=1); self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens // 2, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor: x = F.relu(self._conv_1(inputs)); x = self._residual_stack(x); x = F.relu(self._conv_trans_1(x)); x_recon = self._conv_trans_2(x); return x_recon\nclass VQVAE(nn.Module):\n    def __init__(self, cfg: Config): super(VQVAE, self).__init__(); self.cfg = cfg; self._encoder = Encoder(in_channels=1, num_hiddens=cfg.num_hiddens, num_residual_layers=cfg.num_residual_layers, num_residual_hiddens=cfg.num_residual_hiddens); self._pre_vq_conv = nn.Conv2d(in_channels=cfg.num_hiddens, out_channels=cfg.embedding_dim, kernel_size=1, stride=1); self._vq_layer = VectorQuantizer(num_embeddings=cfg.num_embeddings, embedding_dim=cfg.embedding_dim, commitment_cost=cfg.commitment_cost); self._decoder = Decoder(in_channels=cfg.embedding_dim, num_hiddens=cfg.num_hiddens, num_residual_layers=cfg.num_residual_layers, num_residual_hiddens=cfg.num_residual_hiddens, out_channels=1)\n    def forward(self, x: torch.Tensor) -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: z_e = self._encoder(x); z_e_conv = self._pre_vq_conv(z_e); vq_loss, z_q, perplexity, _ = self._vq_layer(z_e_conv); x_recon = self._decoder(z_q); return vq_loss, x_recon, perplexity\n\n\n# --- 7. Training Setup ---\nprint(\"Setting up training components...\")\n\n# --- Initialize Dataset and DataLoader ---\nprint(\"Initializing Dataset and DataLoader from preprocessed data...\")\ntry:\n    expected_spec_shape = (cfg.num_frequencies, cfg.num_time_steps_per_chunk)\n    dataset = PreprocessedSpectrogramDataset(cfg.preprocessed_dir, expected_shape=expected_spec_shape)\n    dataloader = DataLoader(\n        dataset, batch_size=cfg.batch_size, shuffle=True,\n        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=True\n    )\n    print(f\"DataLoader created with {len(dataloader)} batches per epoch.\")\nexcept FileNotFoundError as e: print(f\"FATAL ERROR: {e}\"); raise e\nexcept Exception as e: print(f\"FATAL ERROR initializing dataset/dataloader: {e}\"); traceback.print_exc(); raise e\n\n# --- Initialize Model, Optimizer, Scaler ---\nprint(\"Initializing Model, Optimizer, and Scaler...\")\nmodel = VQVAE(cfg).to(cfg.device)\noptimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, amsgrad=False)\n# Initialize GradScaler BUT check cfg.use_mixed_precision before using\nscaler = torch.amp.GradScaler(cfg.device, enabled=cfg.use_mixed_precision)\nprint(f\"Model created on {cfg.device}. Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# --- Checkpoint Loading Logic ---\n# ******** FORCE START FROM EPOCH 0 **********\nprint(\"Attempting to load checkpoint (but configured to force start from Epoch 0)...\")\nstart_epoch = 0\ntrain_res_recon_error, train_res_perplexity, train_res_vq_loss = [], [], []\ncfg.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n# --- Comment out or skip the loading part ---\n# load_path = cfg.checkpoint_load_path\n# if not load_path:\n#     checkpoint_files = sorted(cfg.checkpoint_dir.glob(\"vqvae_checkpoint_epoch_*.pth\"), reverse=True)\n#     if checkpoint_files:\n#         load_path = checkpoint_files[0]\n#         print(f\"Found latest checkpoint: {load_path}\")\n#\n# if load_path and load_path.exists():\n#     print(f\"Loading checkpoint: {load_path}\")\n#     # ... (loading code would go here) ...\n# else:\n#     if cfg.checkpoint_load_path: print(f\"Specified checkpoint not found: {cfg.checkpoint_load_path}.\")\n#     else: print(f\"No checkpoint found in {cfg.checkpoint_dir}.\")\n\nprint(f\"Forcing start from Epoch {start_epoch}. Previous checkpoints ignored.\")\n# Optional: Clean the checkpoint directory if desired\n# print(\"Optionally cleaning checkpoint directory...\")\n# for f in cfg.checkpoint_dir.glob(\"*.pth\"):\n#     try:\n#         f.unlink()\n#         print(f\"  Deleted {f.name}\")\n#     except OSError as e:\n#         print(f\"  Error deleting {f.name}: {e}\")\n\n\n# --- 8. Training Loop ---\nprint(f\"\\n--- Starting Training from Epoch {start_epoch} / {cfg.num_epochs} ---\")\n\nfor epoch in range(start_epoch, cfg.num_epochs):\n    model.train()\n    epoch_recon_loss, epoch_vq_loss, epoch_perplexity, epoch_total_loss = 0.0, 0.0, 0.0, 0.0\n    batches_processed, load_errors = 0, 0\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n\n    for batch_idx, data in pbar:\n        if data.abs().sum() < 1e-9: load_errors += 1; continue\n        data = data.to(cfg.device)\n        optimizer.zero_grad(set_to_none=True)\n        try:\n            # Use autocast ONLY if use_mixed_precision is True\n            with torch.amp.autocast(device_type=cfg.device, dtype=torch.float16, enabled=cfg.use_mixed_precision):\n                vq_loss, data_recon, perplexity = model(data)\n                # Handle potential shape mismatch\n                if data_recon.shape != data.shape:\n                    min_t = min(data_recon.shape[-1], data.shape[-1])\n                    if data.shape[-1] > min_t: data_for_loss, recon_for_loss = data[..., :min_t], data_recon\n                    else: data_for_loss, recon_for_loss = data, data_recon[..., :min_t]\n                    if batch_idx == 0: print(f\"\\nWarning: Shape mismatch! Recon: {data_recon.shape}, Data: {data.shape}. Loss on T={min_t}\")\n                else:\n                    data_for_loss, recon_for_loss = data, data_recon\n                recon_error = F.mse_loss(recon_for_loss, data_for_loss)\n                loss = recon_error + vq_loss\n\n            # Scale loss ONLY if using mixed precision\n            if cfg.use_mixed_precision:\n                scaler.scale(loss).backward()\n                # Gradient Clipping (BEFORE optimizer step)\n                scaler.unscale_(optimizer) # Unscale gradients before clipping\n                clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_val)\n                scaler.step(optimizer)\n                scaler.update()\n            else: # Standard backward pass if not using mixed precision\n                loss.backward()\n                # Gradient Clipping (BEFORE optimizer step)\n                clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_val)\n                optimizer.step()\n\n            # --- Accumulate Metrics ---\n            # Detach losses before accumulating if they weren't already\n            epoch_recon_loss += recon_error.item()\n            epoch_vq_loss += vq_loss.item()\n            epoch_perplexity += perplexity.item()\n            epoch_total_loss += loss.item() # loss is already detached implicitly after backward\n            batches_processed += 1\n\n            # --- Logging ---\n            log_dict = {'Loss': f'{loss.item():.4f}', 'Recon': f'{recon_error.item():.4f}', 'VQ': f'{vq_loss.item():.4f}', 'Perp': f'{perplexity.item():.2f}', 'Errors': load_errors}\n            if cfg.use_mixed_precision: log_dict['Scale'] = f'{scaler.get_scale():.1f}'\n            if (batch_idx + 1) % cfg.print_every_n_batches == 0 or batch_idx == len(dataloader) - 1:\n                pbar.set_postfix(log_dict)\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower(): print(f\"\\nOOM Error @ batch {batch_idx}. Skipping batch.\"); torch.cuda.empty_cache(); continue\n            else: print(f\"\\nRuntimeError @ batch {batch_idx}: {e}\"); traceback.print_exc(); continue\n        except Exception as e: print(f\"\\nError @ batch {batch_idx}: {e}\"); traceback.print_exc(); continue\n\n    # --- End of Epoch Summary ---\n    # (No changes needed here)\n    if batches_processed > 0:\n        avg_recon_loss = epoch_recon_loss / batches_processed; avg_vq_loss = epoch_vq_loss / batches_processed\n        avg_perplexity = epoch_perplexity / batches_processed; avg_total_loss = epoch_total_loss / batches_processed\n        train_res_recon_error.append(avg_recon_loss); train_res_perplexity.append(avg_perplexity); train_res_vq_loss.append(avg_vq_loss)\n        print(f\"\\nEpoch [{epoch+1}/{cfg.num_epochs}] Summary:\")\n        print(f\"  Avg Total Loss: {avg_total_loss:.4f} | Avg Recon Error: {avg_recon_loss:.4f} | Avg VQ Loss: {avg_vq_loss:.4f} | Avg Perp: {avg_perplexity:.2f}\")\n        if load_errors > 0: print(f\"  Skipped Batches (Load Errors): {load_errors}\")\n    else:\n        print(f\"\\nEpoch [{epoch+1}/{cfg.num_epochs}] - No batches processed.\"); train_res_recon_error.append(float('nan')); train_res_perplexity.append(float('nan')); train_res_vq_loss.append(float('nan'))\n\n\n    # --- Checkpoint Saving ---\n    # (Save scaler state ONLY if using mixed precision)\n    if (epoch + 1) % cfg.save_every_n_epochs == 0 or epoch == cfg.num_epochs - 1:\n        checkpoint_filename = f\"vqvae_checkpoint_epoch_{epoch+1}.pth\"\n        checkpoint_full_path = cfg.checkpoint_dir / checkpoint_filename\n        print(f\"Saving checkpoint: {checkpoint_full_path} ...\")\n        try:\n            save_dict = {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n                         'train_res_recon_error': train_res_recon_error, 'train_res_perplexity': train_res_perplexity, 'train_res_vq_loss': train_res_vq_loss, 'config': cfg}\n            # Save scaler state ONLY if it was used\n            if cfg.use_mixed_precision:\n                save_dict['scaler_state_dict'] = scaler.state_dict()\n            torch.save(save_dict, checkpoint_full_path)\n            print(\"Checkpoint saved.\")\n        except Exception as cs_e: print(f\"Error saving checkpoint: {cs_e}\")\n\nprint(\"\\n--- Training Finished ---\")\n\n# --- 9. Visualization / Evaluation (Optional) ---\n# (No changes needed here)\nprint(\"\\nVisualizing one reconstruction example...\")\n# ... (paste visualization code here) ...\nmodel.eval()\ntry:\n    vis_converter = SpectrogramConverter(cfg, device=cfg.device)\n    vis_dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n    original_data_batch = next(iter(vis_dataloader)).to(cfg.device)\n    if original_data_batch.abs().sum() < 1e-9: print(\"Loaded empty batch for viz. Skipping.\")\n    elif original_data_batch.ndim != 4 or original_data_batch.shape[1] != 1: print(f\"Unexpected viz batch shape: {original_data_batch.shape}. Skipping.\")\n    else:\n        with torch.no_grad(): vq_loss, data_recon_batch, perplexity = model(original_data_batch)\n        original_spec_plot = original_data_batch[0, 0].cpu().numpy(); reconstructed_spec_plot = data_recon_batch[0, 0].cpu().numpy()\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5)); fig.suptitle(f\"Example Reconstruction - Epoch {epoch+1}\", fontsize=16)\n        im_orig = axes[0].imshow(original_spec_plot, aspect='auto', origin='lower', cmap='viridis'); axes[0].set_title(\"Original Spectrogram (Normalized)\"); axes[0].set_xlabel(\"Time Steps\"); axes[0].set_ylabel(\"Mel Bins\"); fig.colorbar(im_orig, ax=axes[0], shrink=0.6, label=\"Normalized Amplitude\")\n        im_recon = axes[1].imshow(reconstructed_spec_plot, aspect='auto', origin='lower', cmap='viridis'); axes[1].set_title(\"Reconstructed Spectrogram (Normalized)\"); axes[1].set_xlabel(\"Time Steps\"); fig.colorbar(im_recon, ax=axes[1], shrink=0.6, label=\"Normalized Amplitude\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n        print(\"\\nAttempting audio reconstruction...\")\n        try:\n            estimated_recon_samples = original_spec_plot.shape[1] * cfg.hop_length\n            print(\"Reconstructing Original Audio...\"); original_audio = vis_converter.audio_segment_from_spectrogram_tensor(original_data_batch[0], target_len=estimated_recon_samples); print(\"Original Audio (Reconstructed):\")\n            print(\"\\nReconstructing VQ-VAE Output Audio...\"); reconstructed_audio = vis_converter.audio_segment_from_spectrogram_tensor(data_recon_batch[0], target_len=estimated_recon_samples); print(\"Reconstructed Audio (From Model):\")\n        except ImportError: print(\"Install 'ffmpeg' or 'libav' for audio export/display.\")\n        except Exception as audio_err: print(f\"Error during audio recon: {audio_err}\"); traceback.print_exc()\nexcept StopIteration: print(\"Could not get viz batch (Dataset empty?).\")\nexcept Exception as viz_err: print(f\"\\nError during visualization: {viz_err}\"); traceback.print_exc()\n\n\n# --- 10. Plot Loss Curves (Optional) ---\n# (No changes needed here)\nprint(\"\\nPlotting training curves...\")\n# ... (paste plotting code here) ...\ntry:\n    epochs_ran = list(range(len(train_res_recon_error)))\n    if epochs_ran:\n        plt.figure(figsize=(12, 8))\n        plt.subplot(3, 1, 1); plt.plot(epochs_ran, train_res_recon_error, marker='o'); plt.title('Reconstruction Error (MSE Loss)'); plt.ylabel('MSE Loss'); plt.grid(True)\n        plt.subplot(3, 1, 2); plt.plot(epochs_ran, train_res_vq_loss, marker='o'); plt.title('VQ Loss (Codebook + Commitment)'); plt.ylabel('VQ Loss'); plt.grid(True)\n        plt.subplot(3, 1, 3); plt.plot(epochs_ran, train_res_perplexity, marker='o'); plt.title('Codebook Perplexity'); plt.xlabel('Epoch'); plt.ylabel('Perplexity'); plt.grid(True)\n        plt.tight_layout(); plt.show()\n    else: print(\"No training data to plot.\")\nexcept Exception as plot_err: print(f\"Error plotting loss curves: {plot_err}\")\n\n\nprint(\"\\n--- Restart Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:42:27.167797Z","iopub.execute_input":"2025-04-23T01:42:27.168106Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Importing libraries...\nSetting up configuration...\nDerived Config: Chunk Samples=44100, n_fft=4410, hop=441, win=1102, T=101\nUsing Device: CUDA\n!! Using Mixed Precision: False !!\nDataLoader Workers: 0\nBatch Size: 256\nGradient Clipping Norm: 1.0\nDefining utilities...\nSkipping Preprocessing step.\nDefining Optimized Dataset...\nDefining VQ-VAE Model...\nSetting up training components...\nInitializing Dataset and DataLoader from preprocessed data...\nFound 61880 preprocessed spectrogram chunks in /kaggle/working/preprocessed_spectrograms.\nDataLoader created with 241 batches per epoch.\nInitializing Model, Optimizer, and Scaler...\nModel created on cuda. Parameters: 723,521\nAttempting to load checkpoint (but configured to force start from Epoch 0)...\nForcing start from Epoch 0. Previous checkpoints ignored.\n\n--- Starting Training from Epoch 0 / 25 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b0c21049f44b3aba88c0caafc0e970"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [1/25] Summary:\n  Avg Total Loss: 0.2515 | Avg Recon Error: 0.1469 | Avg VQ Loss: 0.1045 | Avg Perp: 1.10\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_1.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395a1cdc3ed04df38ec8146a1fe83236"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [2/25] Summary:\n  Avg Total Loss: 0.5492 | Avg Recon Error: 0.0168 | Avg VQ Loss: 0.5324 | Avg Perp: 1.32\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_2.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84cb6c8b68df47ca9c198cfac7d9151e"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [3/25] Summary:\n  Avg Total Loss: 0.0716 | Avg Recon Error: 0.0182 | Avg VQ Loss: 0.0534 | Avg Perp: 1.38\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_3.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614df96041264ae29916f9bbc5b59b49"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n\nEpoch [4/25] Summary:\n  Avg Total Loss: 0.0108 | Avg Recon Error: 0.0035 | Avg VQ Loss: 0.0073 | Avg Perp: 1.92\nSaving checkpoint: /kaggle/working/vqvae_checkpoints/vqvae_checkpoint_epoch_4.pth ...\nCheckpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/25:   0%|          | 0/241 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab8f8d02f8a141cfa6879875eec586da"}},"metadata":{}},{"name":"stdout","text":"\nWarning: Shape mismatch! Recon: torch.Size([256, 1, 128, 100]), Data: torch.Size([256, 1, 128, 101]). Loss on T=100\n","output_type":"stream"}],"execution_count":null}]}